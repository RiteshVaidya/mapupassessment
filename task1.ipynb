{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "authorship_tag": "ABX9TyPX5N99WS9tTqRrg5wK8sfk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiteshVaidya/mapupassessment/blob/main/task1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Unet_daWgL9Z",
        "outputId": "e69dabd3-6168-4d75-f4ee-511b3b76c3c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id_2    801    802    803    804    805    806    807    808    809    821  \\\n",
            "id_1                                                                         \n",
            "801    0.00   2.80   6.00   7.70  11.70  13.40  16.90  19.60  21.00  23.52   \n",
            "802    2.80   0.00   3.40   5.20   9.20  10.90  14.30  17.10  18.50  20.92   \n",
            "803    6.00   3.40   0.00   2.00   6.00   7.70  11.10  13.90  15.30  17.72   \n",
            "804    7.70   5.20   2.00   0.00   4.40   6.10   9.50  12.30  13.70  16.12   \n",
            "805   11.70   9.20   6.00   4.40   0.00   2.00   5.40   8.20   9.60  12.02   \n",
            "806   13.40  10.90   7.70   6.10   2.00   0.00   3.80   6.60   8.00  10.42   \n",
            "807   16.90  14.30  11.10   9.50   5.40   3.80   0.00   2.90   4.30   6.82   \n",
            "808   19.60  17.10  13.90  12.30   8.20   6.60   2.90   0.00   1.70   4.12   \n",
            "809   21.00  18.50  15.30  13.70   9.60   8.00   4.30   1.70   0.00   2.92   \n",
            "821   23.52  20.92  17.72  16.12  12.02  10.42   6.82   4.12   2.92   0.00   \n",
            "822   24.67  22.07  18.87  17.27  13.17  11.57   7.97   5.27   4.07   1.80   \n",
            "823   26.53  23.93  20.73  19.13  15.03  13.43   9.83   7.13   5.93   3.67   \n",
            "824   27.92  25.32  22.12  20.52  16.42   7.80  11.22   8.52   7.32   5.06   \n",
            "825   29.08  26.48  23.28  21.68  17.58  15.98  12.38   9.68   8.48   6.22   \n",
            "826   30.87  28.27  25.07  23.47  19.37  17.77  14.17  11.47  10.27   8.01   \n",
            "827   32.53  29.93  26.73  25.13  21.03  19.43  15.83  13.13  11.93   9.43   \n",
            "829   36.32  33.72  30.52  28.92  24.82  23.22  19.62  16.92  15.72  13.26   \n",
            "830   38.27  35.67  32.47  30.87  26.77  25.17  21.57  18.87  17.67  15.17   \n",
            "831   39.24  36.64  33.44  31.84  27.74  26.14  22.54  19.84  18.64  16.15   \n",
            "\n",
            "id_2    822    823    824    825    826    827    829    830    831  \n",
            "id_1                                                                 \n",
            "801   24.67  26.53  27.92  29.08  30.87  32.53  36.32  38.27  39.24  \n",
            "802   22.07  23.93  25.32  26.48  28.27  29.93  33.72  35.67  36.64  \n",
            "803   18.87  20.73  22.12  23.28  25.07  26.73  30.52  32.47  33.44  \n",
            "804   17.27  19.13  20.52  21.68  23.47  25.13  28.92  30.87  31.84  \n",
            "805   13.17  15.03  16.42  17.58  19.37  21.03  24.82  26.77  27.74  \n",
            "806   11.57  13.43  14.82  15.98  17.77  19.43  23.22  25.17  26.14  \n",
            "807    7.97   9.83  11.22  12.38  14.17  15.83  19.62  21.57  22.54  \n",
            "808    5.27   7.13   8.52   9.68  11.47  13.13  16.92  18.87  19.84  \n",
            "809    4.07   5.93   7.32   8.48  10.27  11.93  15.72  17.67  18.64  \n",
            "821    1.80   3.67   5.06   6.22   8.01   9.43  13.26  15.17  16.15  \n",
            "822    0.00   2.21   3.60   4.76   6.55   8.00  11.81  13.74  14.68  \n",
            "823    2.21   0.00   1.79   2.94   4.74   6.15  10.00  11.89  12.87  \n",
            "824    3.60   1.79   0.00   1.71   3.50   4.92   8.77  10.66  11.64  \n",
            "825    4.76   2.94   1.71   0.00   2.20   3.65   7.46   9.35  10.33  \n",
            "826    6.55   4.74   3.50   2.20   0.00   2.05   5.81   7.71   8.69  \n",
            "827    8.00   6.15   4.92   3.65   2.05   0.00   4.14   6.06   7.04  \n",
            "829   11.81  10.00  21.40   7.46   5.81   4.14   0.00   2.38   3.36  \n",
            "830   13.74  11.89  10.66   0.00   7.71   6.06   2.38   0.00   1.39  \n",
            "831   14.68  12.87  11.64  10.33   8.69   7.04   3.36   1.39   0.00  \n",
            "{30.87: 4, 6.0: 4, 8.0: 4, 18.87: 4, 2.0: 4, 7.7: 4, 9.6: 2, 7.32: 2, 5.93: 2, 18.64: 2, 1.7: 2, 18.5: 2, 17.67: 2, 8.48: 2, 11.1: 2, 15.3: 2, 27.92: 2, 26.53: 2, 39.24: 2, 19.6: 2, 2.8: 2, 11.7: 2, 21.0: 2, 4.14: 2, 21.57: 2, 16.9: 2, 28.27: 2, 19.37: 2, 2.2: 2, 17.77: 2, 7.71: 2, 25.07: 2, 4.3: 2, 11.22: 2, 13.4: 2, 9.83: 2, 22.54: 2, 2.9: 2, 14.3: 2, 5.4: 2, 12.38: 2, 3.8: 2, 29.08: 2, 38.27: 2, 8.69: 2, 1.79: 2, 1.39: 2, 33.44: 2, 17.1: 2, 8.2: 2, 9.68: 2, 6.6: 2, 13.9: 2, 9.2: 2, 26.48: 2, 10.9: 2, 35.67: 2, 3.4: 2, 17.58: 2, 26.77: 2, 15.98: 2, 23.28: 2, 25.17: 2, 26.14: 2, 10.33: 2, 27.74: 2, 12.87: 2, 11.64: 2, 8.52: 2, 25.32: 2, 16.42: 2, 1.71: 2, 10.66: 2, 22.12: 2, 7.13: 2, 36.64: 2, 23.93: 2, 15.03: 2, 2.94: 2, 13.43: 2, 11.89: 2, 20.73: 2, 19.84: 2, 11.47: 2, 4.74: 2, 13.26: 2, 21.03: 2, 11.93: 2, 32.53: 2, 4.92: 2, 6.15: 2, 7.04: 2, 13.13: 2, 29.93: 2, 3.65: 2, 23.52: 2, 19.43: 2, 6.06: 2, 26.73: 2, 16.12: 2, 1.8: 2, 8.01: 2, 6.82: 2, 15.83: 2, 2.05: 2, 25.13: 2, 9.43: 2, 28.92: 2, 11.81: 2, 5.81: 2, 19.62: 2, 15.72: 2, 36.32: 2, 10.0: 2, 3.36: 2, 16.92: 2, 33.72: 2, 24.82: 2, 7.46: 2, 23.22: 2, 2.38: 2, 30.52: 2, 3.5: 2, 2.92: 2, 4.4: 2, 5.2: 2, 21.68: 2, 6.1: 2, 6.55: 2, 7.97: 2, 4.07: 2, 24.67: 2, 3.6: 2, 2.21: 2, 14.68: 2, 5.27: 2, 22.07: 2, 13.17: 2, 4.76: 2, 11.57: 2, 13.74: 2, 14.17: 2, 10.27: 2, 5.06: 2, 32.47: 2, 12.3: 2, 15.17: 2, 3.67: 2, 16.15: 2, 4.12: 2, 20.92: 2, 12.02: 2, 31.84: 2, 10.42: 2, 6.22: 2, 17.72: 2, 17.27: 2, 23.47: 2, 9.5: 2, 13.7: 2, 20.52: 2, 19.13: 2, 21.4: 1, 7.8: 1, 9.35: 1, 14.82: 1, 8.77: 1}\n",
            "[2, 7, 12, 17, 25, 30, 54, 64, 70, 97, 144, 145, 149, 154, 160, 201, 206, 210, 215, 234, 235, 245, 250, 309, 314, 319, 322, 323, 334, 340]\n",
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "    A   B   C\n",
            "0   5  30  10\n",
            "1  24   7   6\n",
            "2   8  40  28\n",
            "Index(['id', 'name', 'id_2', 'startDay', 'startTime', 'endDay', 'endTime',\n",
            "       'able2Hov2', 'able2Hov3', 'able3Hov2', 'able3Hov3', 'able5Hov2',\n",
            "       'able5Hov3', 'able4Hov2', 'able4Hov3'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-6047fb97a182>:162: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  df2 = pd.read_csv('dataset-2.csv', error_bad_lines=False)\n",
            "Skipping line 39515: expected 15 fields, saw 29\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-6047fb97a182>\u001b[0m in \u001b[0;36m<cell line: 185>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;31m# Assuming df is your DataFrame with the provided data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;31m# Replace it with your actual DataFrame if the variable name is different\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m \u001b[0mresult_completeness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_completeness\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-6047fb97a182>\u001b[0m in \u001b[0;36mtime_check\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;31m# Assuming 'id_2_new' is the correct column for merging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mmerged_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'id_2_new'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'id_2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;31m# Convert the timestamp columns to datetime objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m ) -> DataFrame:\n\u001b[0;32m--> 110\u001b[0;31m     op = _MergeOperation(\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1177\u001b[0m                         \u001b[0;31m#  the latter of which will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m                         \u001b[0mlk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m                         \u001b[0mleft_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1180\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1848\u001b[0m             )\n\u001b[1;32m   1849\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1850\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'id_2_new'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def generate_car_matrix(dataset1_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Creates a DataFrame for id combinations from dataset-1.csv.\n",
        "\n",
        "    Args:\n",
        "        dataset1_path (str): Path to dataset-1.csv.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: Matrix generated with 'car' values,\n",
        "                          where 'id_1' and 'id_2' are used as indices and columns respectively.\n",
        "    \"\"\"\n",
        "    # Read dataset-1.csv\n",
        "    df = pd.read_csv(dataset1_path)\n",
        "\n",
        "    # Use pivot_table to handle duplicate entries\n",
        "    car_matrix = df.pivot_table(index='id_1', columns='id_2', values='car', aggfunc='sum', fill_value=0)\n",
        "\n",
        "    return car_matrix\n",
        "\n",
        "# Example usage:\n",
        "dataset1_path = 'dataset-1.csv'\n",
        "result_matrix = generate_car_matrix(dataset1_path)\n",
        "print(result_matrix)\n",
        "\n",
        "\n",
        "#2\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load your dataset-1.csv into a DataFrame\n",
        "dataset1_df = pd.read_csv('dataset-1.csv')\n",
        "\n",
        "def get_type_count(df: pd.DataFrame) -> dict:\n",
        "    \"\"\"\n",
        "    Categorizes 'car' values into types and returns a dictionary of counts.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame)\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with car types as keys and their counts as values.\n",
        "    \"\"\"\n",
        "    # Extract unique car types and count their occurrences\n",
        "    car_counts = df['car'].value_counts().to_dict()\n",
        "\n",
        "    return car_counts\n",
        "\n",
        "# Example usage:\n",
        "result_type_count = get_type_count(dataset1_df)\n",
        "print(result_type_count)\n",
        "\n",
        "\n",
        "#3\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load your dataset-1.csv into a DataFrame\n",
        "dataset1_df = pd.read_csv('dataset-1.csv')\n",
        "\n",
        "def get_bus_indexes(df: pd.DataFrame) -> list:\n",
        "    \"\"\"\n",
        "    Returns the indexes where the 'bus' values are greater than twice the mean.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame)\n",
        "\n",
        "    Returns:\n",
        "        list: List of indexes where 'bus' values exceed twice the mean.\n",
        "    \"\"\"\n",
        "    # Calculate the mean of 'bus' values\n",
        "    bus_mean = df['bus'].mean()\n",
        "\n",
        "    # Get indexes where 'bus' values are greater than twice the mean\n",
        "    bus_indexes = df[df['bus'] > 2 * bus_mean].index.tolist()\n",
        "\n",
        "    return bus_indexes\n",
        "\n",
        "# Example usage:\n",
        "result_bus_indexes = get_bus_indexes(dataset1_df)\n",
        "print(result_bus_indexes)\n",
        "\n",
        "\n",
        "#4\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def filter_routes(df: pd.DataFrame) -> list:\n",
        "    \"\"\"\n",
        "    Filters and returns routes with average 'truck' values greater than 7.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame)\n",
        "\n",
        "    Returns:\n",
        "        list: List of route names with average 'truck' values greater than 7.\n",
        "    \"\"\"\n",
        "    # Calculate the mean 'truck' values for each route\n",
        "    route_avg_truck = df.groupby('route')['truck'].mean()\n",
        "\n",
        "    # Filter routes with average 'truck' values greater than 7\n",
        "    selected_routes = route_avg_truck[route_avg_truck > 7].index.tolist()\n",
        "\n",
        "    return selected_routes\n",
        "\n",
        "# Load your dataset-1.csv into a DataFrame\n",
        "df = pd.read_csv('dataset-1.csv')\n",
        "\n",
        "# Example usage:\n",
        "result_routes = filter_routes(df)\n",
        "print(result_routes)\n",
        "\n",
        "\n",
        "#5\n",
        "import pandas as pd\n",
        "\n",
        "def multiply_matrix(matrix: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Multiplies matrix values with custom conditions.\n",
        "\n",
        "    Args:\n",
        "        matrix (pandas.DataFrame)\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: Modified matrix with values multiplied based on custom conditions.\n",
        "    \"\"\"\n",
        "    # Example custom condition: Multiply values greater than 10 by 2\n",
        "    modified_matrix = matrix.applymap(lambda x: x * 2 if x > 10 else x)\n",
        "\n",
        "    return modified_matrix\n",
        "\n",
        "# Example usage:\n",
        "# Assuming df is your DataFrame representing the matrix\n",
        "# Replace it with your actual DataFrame if the variable name is different\n",
        "df = pd.DataFrame({\n",
        "    'A': [5, 12, 8],\n",
        "    'B': [15, 7, 20],\n",
        "    'C': [10, 6, 14]\n",
        "})\n",
        "\n",
        "result_matrix = multiply_matrix(df)\n",
        "print(result_matrix)\n",
        "\n",
        "\n",
        "#6\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def time_check(df: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Use shared dataset-2 to verify the completeness of the data by checking whether the timestamps for each unique (`id`, `id_2`) pair cover a full 24-hour and 7 days period.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame)\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: return a boolean series\n",
        "    \"\"\"\n",
        "    # Assuming dataset-2 is loaded into df2 DataFrame\n",
        "    # Replace 'path/to/dataset-2.csv' with the actual path to your dataset-2.csv file\n",
        "    df2 = pd.read_csv('dataset-2.csv', error_bad_lines=False)\n",
        "\n",
        "    # Check the column names in the DataFrame from dataset-2\n",
        "    print(df2.columns)\n",
        "\n",
        "    # Assuming 'id_2_new' is the correct column for merging\n",
        "    merged_df = pd.merge(df, df2, left_on='id_2_new', right_on='id_2', how='inner')\n",
        "\n",
        "    # Convert the timestamp columns to datetime objects\n",
        "    merged_df['start_time'] = pd.to_datetime(merged_df['startDay'] + ' ' + merged_df['startTime'])\n",
        "    merged_df['end_time'] = pd.to_datetime(merged_df['endDay'] + ' ' + merged_df['endTime'])\n",
        "\n",
        "    # Calculate the time duration for each entry\n",
        "    duration = merged_df['end_time'] - merged_df['start_time']\n",
        "\n",
        "    # Check if the duration covers a full 24-hour and 7 days period\n",
        "    completeness_check = (duration >= pd.Timedelta(days=7) - pd.Timedelta(seconds=1)) & (duration >= pd.Timedelta(days=1) - pd.Timedelta(seconds=1))\n",
        "\n",
        "    return completeness_check\n",
        "\n",
        "# Example usage:\n",
        "# Assuming df is your DataFrame with the provided data\n",
        "# Replace it with your actual DataFrame if the variable name is different\n",
        "result_completeness = time_check(df)\n",
        "print(result_completeness)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def time_check(df)->pd.Series:\n",
        "    \"\"\"\n",
        "    Use shared dataset-2 to verify the completeness of the data by checking whether the timestamps for each unique (`id`, `id_2`) pair cover a full 24-hour and 7 days period\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame)\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: return a boolean series\n",
        "    \"\"\"\n",
        "    # Write your logic here\n",
        "\n",
        "    return pd.Series()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UK_C85BPggtU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "DiFOFJQ7gNBi"
      }
    }
  ]
}